# -*- coding: utf-8 -*-
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from pyquery.pyquery import PyQuery as pq
from urllib.parse import quote
from selenium import webdriver
import json

option = webdriver.ChromeOptions()
p = r'C:\Users\Administrator\AppData\Local\Google\Chrome\User Data'
option.add_argument('--user-data-dir=' + p)  # 设置成用户自己的数据目录
browser = webdriver.Chrome(executable_path="C:/Users/Administrator/Desktop/999/chromedriver.exe",options=option)  # chrome_options已弃用
wait = WebDriverWait(browser,10)

KEYWORD = 'ipad'  # 爬取对象
RESULT = []  # 爬取结果
MAX_PAGE = 10  # 爬取页数


def index_page(page):
    """
    执行翻页的操作，如果出错，则重新执行

    :param page: 页码

    :return: 无
    """
    print('正在爬取第{}页'.format(page))
    try:
        url = 'https://s.taobao.com/search?q=' + quote(KEYWORD)
        browser.get(url)
        # 保存爬取到的网页源代码，以对比验证是否正确
        # html=browser.page_source
        # with open('taobao.html','w',encoding='utf-8')as file:
        #     file.write(html)

        if page > 1:
            # 等待class属性为'J_Input'的结点加载出来
            input = wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, 'J_Input'))
            )
            # 等待class属性值为'J_Submit'的结点可以被点击
            submit = wait.until(
                EC.element_to_be_clickable((By.CLASS_NAME, 'J_Submit'))
            )
            # 通过JavaScript修改结点的属性值
            browser.execute_script("arguments[0].setAttribute('value','{}');".format(str(page)), input)
            submit.click()
        # 等待网页全部加载完成，使用CSS选择器查找
        wait.until(EC.text_to_be_present_in_element((
            By.CSS_SELECTOR, '#mainsrp-pager li.item.active > span'), str(page)
        ))
        wait.until(EC.presence_of_element_located((
            By.CSS_SELECTOR, '.m-itemlist .items'
        )))
        get_products()
    except TimeoutException:
        index_page(page)


def get_products():
    """
    筛选商品的图片、价格、成交人数等信息

    :return: 无
    """
    html = browser.page_source
    doc = pq(html)
    # 找到本页商品列表的结点
    items = doc('#mainsrp-itemlist .item').items()
    for item in items:
        product = {
            'image': 'https:' + item.find('.pic .img').attr('data-src'),
            'price': item.find('.price').text(),
            'deal': item.find('.deal-cnt').text()[:-3],
            'title': item.find('.title').text(),
            'shop': item.find('.shop').text(),
            'location': item.find('.location').text()
        }
        RESULT.append(product)


def save_to_json():
    """
    将爬取下来的信息以json的形式保存

    :return: 无
    """
    print('\n爬取工作完成，一共{}个商品'.format(len(RESULT)))
    print('开始写入文件taobao_{}.json......'.format(KEYWORD))
    with open('taobao_{}.json'.format(KEYWORD), 'w', encoding='utf-8')as file:
        file.write(json.dumps(RESULT, ensure_ascii=False))


def main():
    """
    遍历每一页

    :return: 无
    """
    for i in range(1, MAX_PAGE + 1):
        index_page(i)
    save_to_json()


if __name__ == '__main__':
    main()
